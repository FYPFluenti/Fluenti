import torch
import transformers
print("âœ… Core libraries loaded successfully!")

# Test PyTorch and GPU
print(f"ğŸ“¦ PyTorch version: {torch.__version__}")
print(f"ğŸ“¦ Transformers version: {transformers.__version__}")
print(f"ğŸ–¥ï¸ CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"ğŸ¯ GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Test basic GPU operations
    device = torch.device("cuda")
    x = torch.randn(1000, 1000).to(device)
    y = torch.randn(1000, 1000).to(device)
    z = torch.matmul(x, y)
    print(f"âœ… GPU computation test passed! Result shape: {z.shape}")

# Test Transformers library
print("\nğŸ§ª Testing Transformers library...")
try:
    from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
    print("âœ… Speech model classes imported successfully!")
    
    # Test emotion detection model (we know this works)
    from transformers import pipeline
    print("âœ… Pipeline API imported successfully!")
    
    print("\nğŸ‰ All core ML libraries are working perfectly!")
    print("ğŸš€ Ready for direct Whisper model loading!")
    
except Exception as e:
    print(f"âŒ Error testing transformers: {e}")

# Check for other useful packages
optional_packages = ['datasets', 'accelerate', 'soundfile', 'whisper']
print(f"\nğŸ“‹ Optional package status:")
for package in optional_packages:
    try:
        __import__(package)
        print(f"âœ… {package}: Available")
    except ImportError:
        print(f"âŒ {package}: Not available")
