import torch
import transformers
from TTS.api import TTS
print("âœ… All libraries loaded successfully!")

# Test core functionality
print(f"ğŸ“¦ PyTorch version: {torch.__version__}")
print(f"ğŸ“¦ Transformers version: {transformers.__version__}")
print(f"ğŸ–¥ï¸ CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"ğŸ¯ GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# Test TTS initialization
print("\nğŸ§ª Testing TTS initialization...")
try:
    # Use a lightweight model for testing
    tts = TTS("tts_models/en/jenny/jenny", progress_bar=False)
    print("âœ… TTS model loaded successfully!")
except Exception as e:
    print(f"âš ï¸ TTS model loading failed: {e}")
    print("ğŸ’¡ This is normal - TTS models are large and may need internet connection")

print("\nğŸ‰ Core ML setup is complete and ready for FLUENTI Phase 1!")
print("ğŸš€ Ready to proceed with direct Whisper model loading!")
