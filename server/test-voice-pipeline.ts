import { transcribeAudio } from './services/speechService';
import { detectEmotionFromText, detectEmotionFromAudio, combineEmotions } from './services/emotionService';
import fs from 'fs';
import path from 'path';

// Function to set PATH environment variable for Python process
function setFFmpegPath() {
  const ffmpegPath = path.join(process.env.LOCALAPPDATA!, 'Microsoft', 'WinGet', 'Packages', 'Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe', 'ffmpeg-7.1.1-full_build', 'bin');
  if (!process.env.PATH?.includes(ffmpegPath)) {
    process.env.PATH = `${process.env.PATH};${ffmpegPath}`;
  }
}

async function testVoicePipelineComponents() {
  console.log('ğŸ”§ Phase 2 Voice Pipeline Component Test\n');
  
  try {
    setFFmpegPath();
    
    // Test both audio files
    const audioFiles = [
      { name: 'Anxious Test', file: 'english_test.wav', expected: 'anxious' },
      { name: 'Stressed Test', file: 'stressed_test.wav', expected: 'stressed' }
    ];
    
    for (const { name, file, expected } of audioFiles) {
      console.log(`\nğŸ“‹ Testing: ${name} (${file})`);
      console.log('=' .repeat(50));
      
      const audioPath = path.join(process.cwd(), 'server', 'test-audio', file);
      if (!fs.existsSync(audioPath)) {
        console.log(`âŒ ${file} not found`);
        continue;
      }
      
      const audioBuffer = fs.readFileSync(audioPath);
      console.log(`ğŸ“ Loaded: ${audioBuffer.length} bytes`);
      
      // Step 1: STT Test
      console.log('\nğŸ¤ Step 1: Speech-to-Text');
      const sttStart = Date.now();
      try {
        const transcription = await transcribeAudio(audioBuffer, 'en');
        const sttTime = Date.now() - sttStart;
        console.log(`âœ… Transcription: "${transcription}"`);
        console.log(`â±ï¸ STT Time: ${(sttTime / 1000).toFixed(2)}s`);
        
        // Step 2: Text Emotion Detection
        console.log('\nğŸ˜Š Step 2: Text Emotion Analysis');
        const textEmotionStart = Date.now();
        const textEmotion = await detectEmotionFromText(transcription, 'en');
        const textEmotionTime = Date.now() - textEmotionStart;
        console.log(`âœ… Text Emotion: ${textEmotion.emotion} (score: ${textEmotion.score?.toFixed(3)})`);
        console.log(`â±ï¸ Text Emotion Time: ${textEmotionTime}ms`);
        
        // Step 3: Voice Emotion Detection
        console.log('\nğŸµ Step 3: Voice Emotion Analysis');
        const voiceEmotionStart = Date.now();
        let voiceEmotion = null;
        try {
          voiceEmotion = await detectEmotionFromAudio(audioBuffer, 'en');
          const voiceEmotionTime = Date.now() - voiceEmotionStart;
          console.log(`âœ… Voice Emotion: ${voiceEmotion.emotion} (score: ${voiceEmotion.score?.toFixed(3)})`);
          console.log(`â±ï¸ Voice Emotion Time: ${voiceEmotionTime}ms`);
        } catch (error) {
          console.log(`âš ï¸ Voice emotion failed: ${error.message}`);
        }
        
        // Step 4: Combined Emotion
        console.log('\nğŸ”— Step 4: Combined Emotion Analysis');
        let finalEmotion = textEmotion;
        if (voiceEmotion) {
          finalEmotion = combineEmotions(textEmotion, voiceEmotion);
          console.log(`âœ… Combined Emotion: ${finalEmotion.emotion} (score: ${finalEmotion.score?.toFixed(3)})`);
          console.log(`ğŸ¯ Combination: Text(${textEmotion.emotion}) + Voice(${voiceEmotion.emotion}) = ${finalEmotion.emotion}`);
        } else {
          console.log(`âœ… Using Text-Only: ${finalEmotion.emotion} (score: ${finalEmotion.score?.toFixed(3)})`);
        }
        
        // Step 5: Validation
        console.log('\nğŸ“Š Validation Results');
        const transcriptionValid = transcription.toLowerCase().includes(expected.toLowerCase()) ||
                                 transcription.toLowerCase().includes(expected === 'stressed' ? 'stress' : expected);
        const emotionValid = finalEmotion.emotion.toLowerCase().includes(expected.toLowerCase()) ||
                           finalEmotion.emotion.toLowerCase().includes(expected === 'stressed' ? 'stress' : expected) ||
                           (expected === 'anxious' && ['anxiety', 'fear', 'nervous'].includes(finalEmotion.emotion.toLowerCase()));
        
        console.log(`ğŸ¯ Expected: "${expected}"`);
        console.log(`âœ… STT Accuracy: ${transcriptionValid ? 'PASS' : 'FAIL'}`);
        console.log(`âœ… Emotion Accuracy: ${emotionValid ? 'PASS' : `FAIL (got: ${finalEmotion.emotion})`}`);
        console.log(`âœ… Pipeline Health: ${transcriptionValid && emotionValid ? 'EXCELLENT' : 'NEEDS REVIEW'}`);
        
      } catch (error) {
        console.log(`âŒ STT Failed: ${error.message}`);
      }
    }
    
    console.log('\nğŸ Voice Pipeline Component Test Complete');
    
  } catch (error) {
    console.error('âŒ Component test failed:', error);
  }
}

async function testSystemCapabilities() {
  console.log('\nğŸ” System Capabilities Check\n');
  console.log('=' .repeat(50));
  
  // Python Environment Check
  console.log('ğŸ Python Environment:');
  console.log(`   Virtual Env: ${process.cwd()}/.venv/Scripts/python.exe`);
  
  // Audio Files Check
  console.log('\nğŸµ Audio Test Files:');
  const testDir = path.join(process.cwd(), 'server', 'test-audio');
  if (fs.existsSync(testDir)) {
    const files = fs.readdirSync(testDir).filter(f => f.endsWith('.wav'));
    files.forEach(file => {
      const filePath = path.join(testDir, file);
      const stats = fs.statSync(filePath);
      console.log(`   âœ… ${file}: ${(stats.size / 1024).toFixed(1)}KB`);
    });
  }
  
  // FFmpeg Check
  console.log('\nâš™ï¸ FFmpeg Configuration:');
  const ffmpegPath = path.join(process.env.LOCALAPPDATA!, 'Microsoft', 'WinGet', 'Packages', 'Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe', 'ffmpeg-7.1.1-full_build', 'bin');
  console.log(`   Path: ${ffmpegPath}`);
  console.log(`   Available: ${fs.existsSync(ffmpegPath + '/ffmpeg.exe') ? 'YES' : 'NO'}`);
  
  // Model Cache Check  
  console.log('\nğŸ¤– AI Model Cache:');
  const cacheDir = path.join(process.cwd(), 'models', 'hf_cache', 'hub');
  if (fs.existsSync(cacheDir)) {
    const models = fs.readdirSync(cacheDir).filter(d => d.includes('whisper'));
    models.forEach(model => {
      console.log(`   âœ… ${model}`);
    });
  } else {
    console.log('   âš ï¸ Cache directory not found');
  }
  
  console.log('\nğŸŠ System Capability Check Complete');
}

// Run both tests
async function runAllTests() {
  await testSystemCapabilities();
  await testVoicePipelineComponents();
}

runAllTests();
