import { useState, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { ThreeAvatar } from '@/components/ui/three-avatar';
import { useSpeechRecognition } from '@/hooks/useSpeechRecognition';

interface Message {
  user: string;
  ai: string;
  emotion?: string;
  timestamp?: number;
}

interface EmotionalVoiceChatProps {
  language: 'english' | 'urdu';
}

const EmotionalVoiceChat = ({ language }: EmotionalVoiceChatProps) => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [currentEmotion, setCurrentEmotion] = useState('');
  const [currentResponse, setCurrentResponse] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const { startRecording, stopRecording, isRecording } = useSpeechRecognition();

  const handleVoiceInput = async (audioBlob: Blob) => {
    setIsLoading(true);
    try {
      const formData = new FormData();
      formData.append('mode', 'voice');
      formData.append('language', language === 'urdu' ? 'ur' : 'en');
      formData.append('audio', audioBlob);
      formData.append('history', JSON.stringify(messages));

      const response = await fetch('/api/emotional-support', {
        method: 'POST',
        body: formData,
      });

      const data = await response.json();
      
      if (data.response) {
        const newMessage: Message = {
          user: data.transcription || 'Voice input...',
          ai: data.response,
          emotion: data.emotion,
          timestamp: Date.now(),
        };

        setMessages(prev => [...prev, newMessage]);
        setCurrentEmotion(data.emotion || '');
        setCurrentResponse(data.response);

        // Debug logging
        console.log('New AI response set:', data.response);
        console.log('Speech synthesis available:', !!window.speechSynthesis);
        
        // The ThreeAvatar component should handle speaking automatically
        if (data.response) {
          console.log('AI will speak:', data.response);
        }
      }
    } catch (error) {
      console.error('Voice processing error:', error);
      setCurrentResponse(language === 'urdu' 
        ? 'Ù…Ø¹Ø°Ø±ØªØŒ Ú©ÙˆØ¦ÛŒ Ø®Ø±Ø§Ø¨ÛŒ ÛÙˆØ¦ÛŒ ÛÛ’' 
        : 'Sorry, there was an error processing your voice.'
      );
    } finally {
      setIsLoading(false);
    }
  };

  const handleRecordingToggle = async () => {
    if (isRecording) {
      const audioBlob = await stopRecording();
      if (audioBlob) {
        await handleVoiceInput(audioBlob);
      }
    } else {
      startRecording();
    }
  };

  const testSpeak = () => {
    const testMessage = language === 'urdu' 
      ? 'ÛŒÛ Ø§ÛŒÚ© Ù¹ÛŒØ³Ù¹ Ù…ÛŒØ³Ø¬ ÛÛ’' 
      : 'This is a test message';
    setCurrentResponse(testMessage);
  };

  const handleAvatarSpeak = (text: string) => {
    console.log('Avatar speaking:', text);
    
    // Ensure audio context is enabled (user gesture required)
    if (window.speechSynthesis) {
      // Small delay to ensure proper audio context
      setTimeout(() => {
        console.log('Speech synthesis ready');
      }, 100);
    }
  };

  return (
    <div className="flex flex-col h-full max-w-4xl mx-auto p-6">
      {/* Header */}
      <div className="text-center mb-6">
        <h2 className="text-2xl font-bold">
          {language === 'urdu' 
            ? 'Ø¢ÙˆØ§Ø² Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¬Ø°Ø¨Ø§ØªÛŒ Ù…Ø¯Ø¯' 
            : 'Voice Emotional Support'
          }
        </h2>
        {currentEmotion && (
          <p className="text-sm text-gray-600 mt-2">
            <span className="font-semibold">
              {language === 'urdu' ? 'Ù…ÙˆØ¬ÙˆØ¯Û Ø¬Ø°Ø¨Ø§Øª: ' : 'Current Emotion: '}
            </span>
            {currentEmotion}
          </p>
        )}
      </div>

      {/* Avatar Section */}
      <div className="flex justify-center mb-6">
        <ThreeAvatar
          currentMessage={currentResponse}
          language={language}
          isListening={isRecording}
          onSpeak={handleAvatarSpeak}
          className="w-64 h-64"
        />
      </div>

      {/* Voice Controls */}
      <div className="flex justify-center mb-6 space-x-4">
        <Button 
          onClick={handleRecordingToggle}
          disabled={isLoading}
          size="lg"
          className={`px-8 py-4 text-lg font-semibold ${
            isRecording 
              ? 'bg-red-500 hover:bg-red-600 animate-pulse' 
              : 'bg-blue-500 hover:bg-blue-600'
          }`}
        >
          {isLoading
            ? (language === 'urdu' ? 'Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯...' : 'Processing...')
            : isRecording
              ? (language === 'urdu' ? 'Ø±Ú© Ø¬Ø§Ø¦ÛŒÚº ğŸ›‘' : 'Stop Speaking ğŸ›‘')
              : (language === 'urdu' ? 'Ø¨ÙˆÙ„Ù†Ø§ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº ğŸ¤' : 'Start Speaking ğŸ¤')
          }
        </Button>
        
        <Button
          onClick={testSpeak}
          variant="outline"
          size="lg"
          className="px-4 py-4"
        >
          {language === 'urdu' ? 'Ø¢ÙˆØ§Ø² Ù¹ÛŒØ³Ù¹ ğŸ”Š' : 'Test Voice ğŸ”Š'}
        </Button>
      </div>

      {/* Status Indicator */}
      {isRecording && (
        <div className="text-center mb-4">
          <p className="text-red-500 font-semibold animate-pulse">
            {language === 'urdu' ? 'ğŸ¤ Ø³Ù† Ø±ÛØ§ ÛÛ’...' : 'ğŸ¤ Listening...'}
          </p>
        </div>
      )}

      {/* Conversation History */}
      <div className="flex-1 overflow-y-auto space-y-4 bg-gray-50 rounded-lg p-4">
        {messages.length === 0 ? (
          <div className="text-center text-gray-500 py-8">
            <p className="text-lg">
              {language === 'urdu' 
                ? 'Ø¨Ø§Øª Ú†ÛŒØª Ø´Ø±ÙˆØ¹ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø§Ø¦ÛŒÚ© Ø¨Ù¹Ù† Ø¯Ø¨Ø§Ø¦ÛŒÚº' 
                : 'Press the microphone button to start talking'
              }
            </p>
          </div>
        ) : (
          messages.map((message, index) => (
            <div key={index} className="space-y-3">
              {/* User Message */}
              <div className="flex justify-end">
                <div className="max-w-xs lg:max-w-md px-4 py-2 bg-blue-500 text-white rounded-lg shadow">
                  <p className="font-medium">
                    {language === 'urdu' ? 'Ø¢Ù¾:' : 'You:'}
                  </p>
                  <p>{message.user}</p>
                  {message.emotion && (
                    <p className="text-xs mt-1 opacity-80">
                      {language === 'urdu' ? 'Ø¬Ø°Ø¨Ø§Øª:' : 'Emotion:'} {message.emotion}
                    </p>
                  )}
                </div>
              </div>
              
              {/* AI Response */}
              <div className="flex justify-start">
                <div className="max-w-xs lg:max-w-md px-4 py-2 bg-green-500 text-white rounded-lg shadow">
                  <p className="font-medium">
                    {language === 'urdu' ? 'AI Ù…Ø¯Ø¯Ú¯Ø§Ø±:' : 'AI Helper:'}
                  </p>
                  <p>{message.ai}</p>
                </div>
              </div>
            </div>
          ))
        )}
      </div>

      {/* Instructions */}
      <div className="mt-4 text-center text-sm text-gray-600">
        <p>
          {language === 'urdu'
            ? 'Ù…Ø§Ø¦ÛŒÚ© Ø¨Ù¹Ù† Ø¯Ø¨Ø§ Ú©Ø± Ø¨ÙˆÙ„ÛŒÚºØŒ Ù¾Ú¾Ø± Ø¯ÙˆØ¨Ø§Ø±Û Ø¯Ø¨Ø§ Ú©Ø± Ø±Ú© Ø¬Ø§Ø¦ÛŒÚº'
            : 'Press the microphone button to speak, then press again to stop'
          }
        </p>
      </div>
    </div>
  );
};

export { EmotionalVoiceChat };
