# Demonstrate HF_HOME Cache Auto-Loading
import os
from dotenv import load_dotenv

print("ğŸš€ FLUENTI Model Cache Auto-Loading Demo")
print("=" * 50)

# Load environment variables
load_dotenv()

# Verify cache configuration
hf_home = os.getenv('HF_HOME')
whisper_cache = os.getenv('WHISPER_CACHE_DIR')

print(f"ğŸ”§ Configuration:")
print(f"   HF_HOME: {hf_home}")
print(f"   WHISPER_CACHE_DIR: {whisper_cache}")
print()

# Test 1: HuggingFace Transformers Model Download
print("ğŸ“¦ Test 1: HuggingFace Model Auto-Loading")
try:
    from transformers import pipeline
    
    print("Loading sentiment analysis model (will cache on first download)...")
    # This will download to E:\Fluenti\models\hf_cache\hub
    classifier = pipeline("sentiment-analysis", 
                         model="cardiffnlp/twitter-roberta-base-sentiment-latest")
    
    # Test the model
    result = classifier("I love using FLUENTI for emotional therapy!")
    print(f"âœ… Model loaded successfully!")
    print(f"   Test result: {result[0]['label']} (confidence: {result[0]['score']:.3f})")
    
    # Check cache contents
    cache_size = sum(os.path.getsize(os.path.join(dirpath, filename))
                    for dirpath, dirnames, filenames in os.walk(hf_home)
                    for filename in filenames)
    print(f"ğŸ“‚ Cache size: {cache_size / (1024*1024):.1f} MB")
    
except Exception as e:
    print(f"âš ï¸  Model loading error: {e}")

print()

# Test 2: OpenAI Whisper Model Download  
print("ğŸ¤ Test 2: OpenAI Whisper Model Auto-Loading")
try:
    import whisper
    
    print("Loading Whisper 'tiny' model (will cache on first download)...")
    # This will use the WHISPER_CACHE_DIR
    model = whisper.load_model("tiny")
    
    print(f"âœ… Whisper model loaded successfully!")
    print(f"   Model type: {type(model).__name__}")
    print(f"   Model device: {model.device}")
    
    # Check whisper cache
    if os.path.exists(whisper_cache):
        whisper_files = os.listdir(whisper_cache)
        print(f"ğŸ“‚ Whisper cache files: {len(whisper_files)} files")
        for file in whisper_files:
            if file.endswith('.pt'):
                size = os.path.getsize(os.path.join(whisper_cache, file)) / (1024*1024)
                print(f"   â€¢ {file}: {size:.1f} MB")
    
except Exception as e:
    print(f"âš ï¸  Whisper loading error: {e}")

print()
print("ğŸ¯ CACHE BENEFITS:")
print("âœ… Models download once, reuse instantly")
print("âœ… No repeated downloads - faster startup")
print("âœ… Organized model storage in project folder")
print("âœ… Easy to backup/share models with project")
print("âœ… Reduced internet bandwidth usage")

print(f"\nğŸ“ Your models are cached in: {hf_home}")
print("ğŸš€ FLUENTI is ready for instant model loading!")
