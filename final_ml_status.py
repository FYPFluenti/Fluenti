# FLUENTI Phase 1: Final ML Library Installation Status
print("üéØ FLUENTI Phase 1: FINAL ML Library Installation Status")
print("=" * 65)

import sys
print(f"Python: {sys.version.split()[0]}")
print(f"Virtual Environment: {sys.prefix}")

print(f"\nüöÄ SUCCESSFULLY INSTALLED LIBRARIES:")
print("-" * 50)

# Test all your requested packages
success_count = 0
total_count = 0

packages_to_test = [
    ("PyTorch (CUDA)", "torch", "CUDA support"),
    ("Transformers", "transformers", "HuggingFace models"),
    ("Datasets", "datasets", "Dataset loading"),
    ("PEFT", "peft", "Parameter efficient fine-tuning"),
    ("TRL", "trl", "Transformer reinforcement learning"),
    ("Accelerate", "accelerate", "Distributed training"),
    ("BitsAndBytes", "bitsandbytes", "Quantization"),
    ("SoundFile", "soundfile", "Audio I/O"),
    ("PyAnnote Audio", "pyannote.audio", "Audio analysis"),
    ("ONNX", "onnx", "Model optimization"),
    ("ONNX Runtime GPU", "onnxruntime", "GPU inference"),
    ("Faster Whisper", "faster_whisper", "Fast STT"),
    ("FFmpeg Python", "ffmpeg", "Audio processing")
]

for display_name, import_name, description in packages_to_test:
    total_count += 1
    try:
        if import_name == "torch":
            import torch
            version = torch.__version__
            extra = f"CUDA: {torch.cuda.is_available()}"
            print(f"‚úÖ {display_name:<20} {version:<15} | {description} ({extra})")
        elif import_name == "onnxruntime":
            import onnxruntime
            version = onnxruntime.__version__
            print(f"‚úÖ {display_name:<20} {version:<15} | {description}")
        else:
            module = __import__(import_name)
            version = getattr(module, '__version__', 'Unknown')
            print(f"‚úÖ {display_name:<20} {version:<15} | {description}")
        success_count += 1
    except ImportError:
        print(f"‚ùå {display_name:<20} {'Not Available':<15} | {description}")
    except Exception as e:
        print(f"‚ö†Ô∏è  {display_name:<20} {'Error':<15} | {description}")

# Test GPU functionality in detail
print(f"\nüñ•Ô∏è GPU CONFIGURATION:")
print("-" * 30)
try:
    import torch
    if torch.cuda.is_available():
        print(f"‚úÖ GPU Model: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ CUDA Version: {torch.version.cuda}")
        print(f"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"‚úÖ PyTorch Version: {torch.__version__}")
        
        # Test GPU computation
        device = torch.device("cuda")
        x = torch.randn(1000, 1000).to(device)
        y = torch.matmul(x, x.T)
        print(f"‚úÖ GPU Computation: Working")
    else:
        print(f"‚ùå CUDA not available")
except Exception as e:
    print(f"‚ùå GPU test failed: {e}")

# Test audio processing pipeline
print(f"\nüéµ AUDIO PROCESSING PIPELINE:")
print("-" * 35)
try:
    import soundfile as sf
    from transformers import AutoProcessor
    import numpy as np
    
    # Test audio creation
    sample_rate = 16000
    audio_data = np.random.randn(sample_rate * 2).astype(np.float32)
    print(f"‚úÖ Audio Generation: {audio_data.shape}")
    
    # Test Whisper preprocessing
    processor = AutoProcessor.from_pretrained("openai/whisper-tiny")
    inputs = processor(audio_data, sampling_rate=sample_rate, return_tensors="pt")
    print(f"‚úÖ Whisper Preprocessing: {inputs['input_features'].shape}")
    
    # Test faster-whisper
    from faster_whisper import WhisperModel
    print(f"‚úÖ Faster Whisper: Available")
    
except Exception as e:
    print(f"‚ùå Audio pipeline test failed: {e}")

# Missing packages and alternatives
print(f"\n‚ö†Ô∏è MISSING PACKAGES & SOLUTIONS:")
print("-" * 35)
missing_solutions = [
    ("TTS", "Use Windows Speech API or install coqui-ai TTS"),
    ("OpenAI Whisper", "Use faster-whisper (installed) or transformers"),
    ("espeak-ng", "Install Windows binary separately"),
    ("readyplayer.me", "Invalid package name - not needed")
]

for package, solution in missing_solutions:
    print(f"‚Ä¢ {package:<15}: {solution}")

# Final status
print(f"\nüìä INSTALLATION SUMMARY:")
print("=" * 30)
print(f"‚úÖ Successfully installed: {success_count}/{total_count} packages")
print(f"üîß Missing/Alternative: {total_count - success_count}/{total_count} packages")

if success_count >= 10:
    status = "üéâ EXCELLENT"
    color = "GREEN"
elif success_count >= 8:
    status = "‚úÖ GOOD"
    color = "YELLOW"
else:
    status = "‚ö†Ô∏è PARTIAL"
    color = "RED"

print(f"\n{status} - Your FLUENTI ML environment status!")

print(f"\nüöÄ READY FOR FLUENTI PHASE 1:")
print("‚úÖ Direct Whisper model loading")  
print("‚úÖ GPU-accelerated inference")
print("‚úÖ Audio processing pipeline")
print("‚úÖ Emotion detection models")
print("‚úÖ Advanced ML training tools")

print(f"\nüí° NEXT STEPS:")
print("1. Test direct Whisper model loading")
print("2. Implement FLUENTI STT integration") 
print("3. Create emotion detection pipeline")
print("4. Deploy to production")

print(f"\nüéØ Your system is ready for advanced ML development!")
